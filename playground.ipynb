{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "playground.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMpeU+LKg5NOH0Or2yK2m82",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markaaronslater/NMT/blob/master/playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjKS-XepHBMy",
        "outputId": "fc3fb823-1d0f-452d-a379-1dcfaf838eb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkGwarO9-62Z",
        "outputId": "a8f7daec-54f9-4db0-d4e1-01804fe0e819",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjK38odg-8jA",
        "outputId": "f6cd3ab9-5ae3-4273-b809-bc7ecc9ff1cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "!pip install subword-nmt # for segmenting words into subwords\n",
        "!pip install stanza # for tokenizing corpus and tagging with morphological data\n",
        "!pip install sacremoses # for detokenizing model predictions\n",
        "!pip install sacrebleu # for evaluation\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting subword-nmt\n",
            "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
            "Installing collected packages: subword-nmt\n",
            "Successfully installed subword-nmt-0.3.7\n",
            "Collecting stanza\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/8b/3a9e7a8d8cb14ad6afffc3983b7a7322a3a24d94ebc978a70746fcffc085/stanza-1.1.1-py3-none-any.whl (227kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 11.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (50.3.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n",
            "Installing collected packages: stanza\n",
            "Successfully installed stanza-1.1.1\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses) (0.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses) (4.41.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=20eff72477284e0d88f65b0463ad0bb4a3276c78fd7e3864f1649bde3d84a74b\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.43\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/c4/8e948f601a4f9609e8b2b58f31966cb13cf17b940b82aa3e767f01c42c52/sacrebleu-1.4.14-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.3MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Installing collected packages: portalocker, sacrebleu\n",
            "Successfully installed portalocker-2.0.0 sacrebleu-1.4.14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzAGPwIj--Jp",
        "outputId": "5639a641-dc38-4c91-d705-7ecdd9bbc0c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "# make sure using GPU\n",
        "# (Runtime -> Change runtime type -> Hardware accelerator = GPU).\n",
        "!nvidia-smi\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Nov  6 00:38:25 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.32.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5h17Mnv-_pm",
        "outputId": "2e8ae8dc-8414-49fd-d7a6-29c9de989c56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "%cd /content/gdrive/My Drive/NMT\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/NMT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRs8rx7V_Blx"
      },
      "source": [
        "\n",
        "from src.model_utils import load_pretrained\n",
        "from src.translate import translate\n",
        "from src.predict import predict\n",
        "from src.evaluate import evaluate\n",
        "from src.preprocessing.corpus_utils import read_corpus\n",
        "\n",
        "import stanza\n",
        "from subword_nmt.apply_bpe import BPE\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKY4XS-W_Dfh"
      },
      "source": [
        "# recommended path to project root directory: place cloned NMT folder in 'My Drive' folder of Google Drive account:\n",
        "path = '/content/gdrive/My Drive/NMT/'\n",
        "model_name = 'old_replica/' # name of pre-trained model to load\n",
        "checkpoint_path = path + 'checkpoints/' + model_name\n",
        "corpus_path = path + 'corpuses/iwslt16_en_de/subword_segmented/'\n",
        "### ??how to give them access to my pre-trained model?? too large to commit to github??\n",
        "\n",
        "translator, model_data = load_pretrained(checkpoint_path=checkpoint_path)\n",
        "#test_batches = model_data[\"test_batches\"]\n",
        "src_word_to_idx = model_data[\"src_word_to_idx\"]\n",
        "idx_to_trg_word = model_data[\"idx_to_trg_word\"]\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqdhNJwk_GG4",
        "outputId": "74f89068-d97b-432c-b191-887c4301c96a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "stanza.download(lang='de', processors='tokenize,mwt,pos')\n",
        "stanza_de_processor = stanza.Pipeline(lang='de', processors='tokenize,mwt,pos', tokenize_no_ssplit=True, tokenize_batch_size=64, mwt_batch_size=200, pos_batch_size=10000)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 30.4MB/s]                    \n",
            "2020-11-06 00:54:31 INFO: Downloading these customized packages for language: de (German)...\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | gsd     |\n",
            "| mwt       | gsd     |\n",
            "| pos       | gsd     |\n",
            "| pretrain  | gsd     |\n",
            "=======================\n",
            "\n",
            "2020-11-06 00:54:31 INFO: File exists: /root/stanza_resources/de/tokenize/gsd.pt.\n",
            "2020-11-06 00:54:31 INFO: File exists: /root/stanza_resources/de/mwt/gsd.pt.\n",
            "2020-11-06 00:54:31 INFO: File exists: /root/stanza_resources/de/pos/gsd.pt.\n",
            "2020-11-06 00:54:31 INFO: File exists: /root/stanza_resources/de/pretrain/gsd.pt.\n",
            "2020-11-06 00:54:31 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
            "2020-11-06 00:54:31 INFO: Loading these models for language: de (German):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | gsd     |\n",
            "| mwt       | gsd     |\n",
            "| pos       | gsd     |\n",
            "=======================\n",
            "\n",
            "2020-11-06 00:54:31 INFO: Use device: gpu\n",
            "2020-11-06 00:54:31 INFO: Loading: tokenize\n",
            "2020-11-06 00:54:32 INFO: Loading: mwt\n",
            "2020-11-06 00:54:32 INFO: Loading: pos\n",
            "2020-11-06 00:54:33 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7YvL_WR_HQd"
      },
      "source": [
        "bpe = BPE(open(corpus_path + 'bpe_codes', 'r'), vocab=set(src_word_to_idx))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zN2qtlOIljr",
        "outputId": "52c79544-f964-42ee-aa58-549458913944",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# a) translation playground:\n",
        "# place any number of whatever German sentences you want as strings inside following list:\n",
        "input = [\"Dies ist ein deutscher Testsatz. Wird das Modell es erfolgreich übersetzen können?\", \n",
        "         \"Wenn nicht, wird diese Demo nicht sehr beeindruckend sein ...\",\n",
        "         \"Ich empfehle, dass Sie zuerst einen englischen Satz erstellen und ihn dann mit Google Translate in Deutsch konvertieren.\"]\n",
        "\n",
        "# determined via Google Translate:\n",
        "sample_targets = [\"This is a German test sentence. Will the model be able to translate it successfully?\",\n",
        "                  \"If it doesn't, then this demo will not be very impressive...\",\n",
        "                  \"I recommend that you first come up with an English sentence, and then use Google Translate to convert it to German.\"]\n",
        "\n",
        "translations = translate(input, stanza_de_processor, translator, src_word_to_idx, idx_to_trg_word, bpe, device='cuda:0', bsz=8)\n",
        "for translation in translations:\n",
        "    print(translation)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenizing, multiword-token-expanding, pos-tagging...\n",
            "truecasing...\n",
            "subword segmenting...\n",
            "converting to batches of indices...\n",
            "making predictions...\n",
            "This is a German test. Will the model be able to translate it?\n",
            "If not, this demo is not very impressive...\n",
            "I suggest that you're going to make a English sentence, and then you put it with Google Translate in German.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUC6xNIBz0q5",
        "outputId": "8ea1a8e2-81ab-4dc1-84ae-dd284ca5af83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# optional - if targets are available, evaluate via BLEU metric:\n",
        "print(evaluate(translations, [sample_targets]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40.87641109454185\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZBxjIipKifR",
        "outputId": "b1126536-641b-4e65-d2fb-9bbeb3943929",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!sacrebleu --list"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The available test sets are:\n",
            "                 wmt20/tworefs: WMT20 news test sets with two references\n",
            "             wmt20/robust/set3: WMT20 robustness task, set 3\n",
            "             wmt20/robust/set2: WMT20 robustness task, set 2\n",
            "             wmt20/robust/set1: WMT20 robustness task, set 1\n",
            "                     wmt20/dev: Development data for tasks new to 2020.\n",
            "                         wmt20: Official evaluation data for WMT20\n",
            "             wmt19/google/wmtp: Additional paraphrase of the official WMT19 reference.\n",
            "              wmt19/google/hqr: Best human selected-reference between wmt19 and wmt19/google/ar.\n",
            "              wmt19/google/hqp: Best human-selected reference between wmt19/google/arp and wmt19/google/wmtp.\n",
            "            wmt19/google/hqall: Best human-selected reference among original official reference and the Google reference and paraphrases.\n",
            "              wmt19/google/arp: Additional paraphrase of wmt19/google/ar.\n",
            "               wmt19/google/ar: Additional high-quality reference for WMT19/en-de.\n",
            "                     wmt19/dev: Development data for tasks new to 2019.\n",
            "                         wmt19: Official evaluation data.\n",
            "                 wmt18/test-ts: Official evaluation sources with extra test sets interleaved.\n",
            "                     wmt18/dev: Development data (Estonian<>English).\n",
            "                         wmt18: Official evaluation data.\n",
            "                 wmt17/tworefs: Systems with two references.\n",
            "                      wmt17/ms: Additional Chinese-English references from Microsoft Research.\n",
            "                wmt17/improved: Improved zh-en and en-zh translations.\n",
            "                     wmt17/dev: Development sets released for new languages in 2017.\n",
            "                       wmt17/B: Additional reference for EN-FI and FI-EN.\n",
            "                         wmt17: Official evaluation data.\n",
            "                 wmt16/tworefs: EN-FI with two references.\n",
            "                     wmt16/dev: Development sets released for new languages in 2016.\n",
            "                       wmt16/B: Additional reference for EN-FI.\n",
            "                         wmt16: Official evaluation data.\n",
            "                         wmt15: Official evaluation data.\n",
            "                    wmt14/full: Evaluation data released after official evaluation for further research.\n",
            "                         wmt14: Official evaluation data.\n",
            "                         wmt13: Official evaluation data.\n",
            "                         wmt12: Official evaluation data.\n",
            "                         wmt11: Official evaluation data.\n",
            "                         wmt10: Official evaluation data.\n",
            "                         wmt09: Official evaluation data.\n",
            "                      wmt08/nc: Official evaluation data (news commentary).\n",
            "                wmt08/europarl: Official evaluation data (Europarl).\n",
            "                         wmt08: Official evaluation data.\n",
            "                 multi30k/2018: 2018 flickr test set of Multi30k dataset. See https://competitions.codalab.org/competitions/19917 for evaluation.\n",
            "                 multi30k/2017: 2017 flickr test set of Multi30k dataset\n",
            "                 multi30k/2016: 2016 flickr test set of Multi30k dataset\n",
            "                      mtnt2019: Test set for the WMT 19 robustness shared task\n",
            "                 mtnt1.1/valid: Validation data for the Machine Translation of Noisy Text task: http://www.cs.cmu.edu/~pmichel1/mtnt/\n",
            "                 mtnt1.1/train: Training data for the Machine Translation of Noisy Text task: http://www.cs.cmu.edu/~pmichel1/mtnt/\n",
            "                  mtnt1.1/test: Test data for the Machine Translation of Noisy Text task: http://www.cs.cmu.edu/~pmichel1/mtnt/\n",
            "               iwslt17/tst2016: Development data for IWSLT 2017.\n",
            "               iwslt17/tst2015: Development data for IWSLT 2017.\n",
            "               iwslt17/tst2014: Development data for IWSLT 2017.\n",
            "               iwslt17/tst2013: Development data for IWSLT 2017.\n",
            "               iwslt17/tst2012: Development data for IWSLT 2017.\n",
            "               iwslt17/tst2011: Development data for IWSLT 2017.\n",
            "               iwslt17/tst2010: Development data for IWSLT 2017.\n",
            "               iwslt17/dev2010: Development data for IWSLT 2017.\n",
            "                       iwslt17: Official evaluation data for IWSLT.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMwQTFJuHCwa",
        "outputId": "52312cac-302d-41f8-a71a-914714306103",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# b) replicate BLEU score on test set\n",
        "\n",
        "\n",
        "\n",
        "# download iwslt17 test set\n",
        "!sacrebleu -t iwslt17 -l de-en --echo src > iwslt17-de-en.src"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sacreBLEU: Downloading https://wit3.fbk.eu/archive/2017-01-ted-test/texts/en/fr/en-fr.tgz to /root/.sacrebleu/iwslt17/en-fr.tgz\n",
            "sacreBLEU: Checksum passed: 1849bcc3b006dc0642a8843b11aa7192\n",
            "sacreBLEU: Extracting /root/.sacrebleu/iwslt17/en-fr.tgz\n",
            "sacreBLEU: Downloading https://wit3.fbk.eu/archive/2017-01-ted-test/texts/fr/en/fr-en.tgz to /root/.sacrebleu/iwslt17/fr-en.tgz\n",
            "sacreBLEU: Checksum passed: 79bf7a2ef02d226875f55fb076e7e473\n",
            "sacreBLEU: Extracting /root/.sacrebleu/iwslt17/fr-en.tgz\n",
            "sacreBLEU: Downloading https://wit3.fbk.eu/archive/2017-01-ted-test/texts/en/de/en-de.tgz to /root/.sacrebleu/iwslt17/en-de.tgz\n",
            "sacreBLEU: Checksum passed: b68e7097b179491f6c466ef41ad72b9b\n",
            "sacreBLEU: Extracting /root/.sacrebleu/iwslt17/en-de.tgz\n",
            "sacreBLEU: Downloading https://wit3.fbk.eu/archive/2017-01-ted-test/texts/de/en/de-en.tgz to /root/.sacrebleu/iwslt17/de-en.tgz\n",
            "sacreBLEU: Checksum passed: e3f5b2a075a2da1a395c8b60bf1e9be1\n",
            "sacreBLEU: Extracting /root/.sacrebleu/iwslt17/de-en.tgz\n",
            "sacreBLEU: Downloading https://wit3.fbk.eu/archive/2017-01-ted-test/texts/en/ar/en-ar.tgz to /root/.sacrebleu/iwslt17/en-ar.tgz\n",
            "sacreBLEU: Checksum passed: ecdc6bc4ab4c8984e919444f3c05183a\n",
            "sacreBLEU: Extracting /root/.sacrebleu/iwslt17/en-ar.tgz\n",
            "sacreBLEU: Downloading https://wit3.fbk.eu/archive/2017-01-ted-test/texts/ar/en/ar-en.tgz to /root/.sacrebleu/iwslt17/ar-en.tgz\n",
            "sacreBLEU: Checksum passed: 4b5141d14b98706c081371e2f8afe0ca\n",
            "sacreBLEU: Extracting /root/.sacrebleu/iwslt17/ar-en.tgz\n",
            "sacreBLEU: Downloading https://wit3.fbk.eu/archive/2017-01-ted-test/texts/en/ja/en-ja.tgz to /root/.sacrebleu/iwslt17/en-ja.tgz\n",
            "sacreBLEU: Checksum passed: d957ee79de1f33c89077d37c5a2c5b06\n",
            "sacreBLEU: Extracting /root/.sacrebleu/iwslt17/en-ja.tgz\n",
            "sacreBLEU: Downloading https://wit3.fbk.eu/archive/2017-01-ted-test/texts/ja/en/ja-en.tgz to /root/.sacrebleu/iwslt17/ja-en.tgz\n",
            "sacreBLEU: Checksum passed: c213e8bb918ebf843543fe9fd2e33db2\n",
            "sacreBLEU: Extracting /root/.sacrebleu/iwslt17/ja-en.tgz\n",
            "sacreBLEU: Downloading https://wit3.fbk.eu/archive/2017-01-ted-test/texts/en/ko/en-ko.tgz to /root/.sacrebleu/iwslt17/en-ko.tgz\n",
            "sacreBLEU: Checksum passed: 59f6a81c707378176e9ad8bb8d811f5f\n",
            "sacreBLEU: Extracting /root/.sacrebleu/iwslt17/en-ko.tgz\n",
            "sacreBLEU: Downloading https://wit3.fbk.eu/archive/2017-01-ted-test/texts/ko/en/ko-en.tgz to /root/.sacrebleu/iwslt17/ko-en.tgz\n",
            "sacreBLEU: Checksum passed: 7e580af973bb389ec1d1378a1850742f\n",
            "sacreBLEU: Extracting /root/.sacrebleu/iwslt17/ko-en.tgz\n",
            "sacreBLEU: Downloading https://wit3.fbk.eu/archive/2017-01-ted-test/texts/en/zh/en-zh.tgz to /root/.sacrebleu/iwslt17/en-zh.tgz\n",
            "sacreBLEU: Checksum passed: 975a858783a0ebec8c57d83ddd5bd381\n",
            "sacreBLEU: Extracting /root/.sacrebleu/iwslt17/en-zh.tgz\n",
            "sacreBLEU: Downloading https://wit3.fbk.eu/archive/2017-01-ted-test/texts/zh/en/zh-en.tgz to /root/.sacrebleu/iwslt17/zh-en.tgz\n",
            "sacreBLEU: Checksum passed: cc51d9b7fe1ff2af858c6a0dd80b8815\n",
            "sacreBLEU: Extracting /root/.sacrebleu/iwslt17/zh-en.tgz\n",
            "sacreBLEU: Processing /root/.sacrebleu/iwslt17/raw/de-en/IWSLT17.TED.tst2017.de-en.de.xml to /root/.sacrebleu/iwslt17/de-en.de\n",
            "sacreBLEU: Processing /root/.sacrebleu/iwslt17/raw/en-de/IWSLT17.TED.tst2017.en-de.en.xml to /root/.sacrebleu/iwslt17/de-en.en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p0QWFZjHh0P",
        "outputId": "d164ab4e-39e2-4fd3-d880-baf17c1666fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_set = read_corpus('iwslt17-de-en.src', path='/content/gdrive/My Drive/NMT/')\n",
        "translations = translate(test_set, stanza_de_processor, translator, src_word_to_idx, idx_to_trg_word, bpe)\n",
        "!cat beam_preds.txt | sacrebleu -t iwslt17 -l de-en"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenizing, multiword-token-expanding, pos-tagging...\n",
            "truecasing...\n",
            "subword segmenting...\n",
            "converting to batches of indices...\n",
            "making predictions...\n",
            "BLEU+case.mixed+lang.de-en+numrefs.1+smooth.exp+test.iwslt17+tok.13a+version.1.4.14 = 21.6 58.4/30.8/17.8/10.5 (BP = 0.899 ratio = 0.904 hyp_len = 18958 ref_len = 20972)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}