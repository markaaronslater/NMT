{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "playground.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOQ8p5liDS8FpZ3lm2elSE8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markaaronslater/NMT/blob/master/playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjKS-XepHBMy"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkGwarO9-62Z"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjK38odg-8jA"
      },
      "source": [
        "!pip install subword-nmt # for segmenting words into subwords\n",
        "!pip install stanza # for tokenizing corpus and tagging with morphological data\n",
        "!pip install sacremoses # for detokenizing model predictions\n",
        "!pip install sacrebleu # for evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzAGPwIj--Jp"
      },
      "source": [
        "# make sure using GPU\n",
        "# (Runtime -> Change runtime type -> Hardware accelerator = GPU).\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5h17Mnv-_pm"
      },
      "source": [
        "%cd /content/gdrive/My Drive/NMT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRs8rx7V_Blx"
      },
      "source": [
        "from src.model_utils import load_pretrained\n",
        "from src.translate import translate\n",
        "from src.predict import predict\n",
        "from src.evaluate import evaluate\n",
        "from src.preprocessing.corpus_utils import read_corpus, get_references\n",
        "\n",
        "import stanza\n",
        "from subword_nmt.apply_bpe import BPE"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKY4XS-W_Dfh"
      },
      "source": [
        "# recommended path to project root directory: place cloned NMT folder in 'My Drive' folder of Google Drive account:\n",
        "path = '/content/gdrive/My Drive/NMT/'\n",
        "model_name = 'relu2/' # name of pre-trained model to load\n",
        "checkpoint_path = path + 'checkpoints/' + model_name\n",
        "#corpus_path = path + 'corpuses/iwslt16_en_de/subword_segmented/'\n",
        "corpus_path = path + 'data/iwslt/en-de/subword_segmented/'\n",
        "\n",
        "translator, model_data = load_pretrained(checkpoint_path=checkpoint_path)\n",
        "src_word_to_idx = model_data[\"src_word_to_idx\"]\n",
        "idx_to_trg_word = model_data[\"idx_to_trg_word\"]\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqdhNJwk_GG4"
      },
      "source": [
        "stanza.download(lang='de', processors='tokenize,mwt,pos')\n",
        "stanza_de_processor = stanza.Pipeline(lang='de', processors='tokenize,mwt,pos', tokenize_no_ssplit=True, tokenize_batch_size=64, mwt_batch_size=200, pos_batch_size=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7YvL_WR_HQd"
      },
      "source": [
        "bpe = BPE(open(corpus_path + 'bpe_codes', 'r'), vocab=set(src_word_to_idx))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zN2qtlOIljr"
      },
      "source": [
        "# a) translation playground:\n",
        "# place any number of whatever German sentences you want as strings inside following list:\n",
        "input = [\"Dies ist ein deutscher Beispielsatz. Wird es richtig Ã¼bersetzt?\", \n",
        "         \"Wenn nicht, wird diese Demo nicht sehr beeindruckend sein ...\",\n",
        "         \"Ich empfehle, dass Sie zuerst einen englischen Satz erstellen und ihn dann mit Google Translate in Deutsch konvertieren.\"]\n",
        "\n",
        "# determined via Google Translate:\n",
        "sample_targets = [\"This is a sample German sentence. Will it be translated correctly?\",\n",
        "                  \"If not, then this demo will not be very impressive...\",\n",
        "                  \"I recommend that you first come up with an English sentence, and then use Google Translate to convert it to German.\"]\n",
        "\n",
        "translations = translate(input, stanza_de_processor, translator, src_word_to_idx, idx_to_trg_word, bpe, device='cuda:0', bsz=8)\n",
        "for translation in translations:\n",
        "    print(translation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUC6xNIBz0q5"
      },
      "source": [
        "# optional - if targets are available, evaluate via BLEU metric:\n",
        "print(evaluate(translations, [sample_targets]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXuSow61U69I"
      },
      "source": [
        "# b) replicate BLEU score on test set\n",
        "### can observe predictions inside <checkpoint_path>/beam_preds.txt\n",
        "test_path = path + 'data/iwslt/en-de/'\n",
        "test_set = read_corpus('test.de', path=test_path)\n",
        "test_references = get_references(path=test_path, dev=False)\n",
        "translations = translate(test_set, stanza_de_processor, translator, src_word_to_idx, idx_to_trg_word, bpe, checkpoint_path)\n",
        "print(evaluate(translations, test_references))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}