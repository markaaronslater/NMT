{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT_driver.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markaaronslater/NMT/blob/master/NMT_driver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qngiJLEPEzWv"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZXApLY8E9xw"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGiBT-uxXleg"
      },
      "source": [
        "!pip install subword-nmt # for segmenting words into subwords\n",
        "!pip install stanza # for tokenizing corpus and tagging with morphological data\n",
        "!pip install sacremoses # for detokenizing model predictions\n",
        "!pip install sacrebleu # for evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVINnIj9E_N0"
      },
      "source": [
        "# make sure using GPU\n",
        "# (Runtime -> Change runtime type -> Hardware accelerator = GPU).\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w_rcYx8JVY5"
      },
      "source": [
        "# recommended: place cloned NMT folder in Google drive folder 'My Drive':\n",
        "path = '/content/gdrive/My Drive/NMT/'\n",
        "corpus_path = path + 'corpuses/iwslt16_en_de/'\n",
        "#corpus_path = path + 'data/iwslt/en-de/'\n",
        "config_path = path + 'configs/'\n",
        "\n",
        "### REQUIRED: ###\n",
        "# create a folder inside of checkpoints, named <model_name>. this will hold all checkpoints for the model, its per-epoch training stats,\n",
        "# and files holding its greedy dev set predictions after each epoch.\n",
        "model_name = 'old_replica_two_layer/' # your model name here. remember to ensure desired settings are set in config files of NMT/configs/ !!!\n",
        "checkpoint_path = path + 'checkpoints/' + model_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgAPTgJxhNGz"
      },
      "source": [
        "%cd /content/gdrive/My Drive/NMT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgyoFSHKJ5Z-"
      },
      "source": [
        "from src.preprocessing.apply_stanza_processors import apply_stanza_processors\n",
        "from src.preprocessing.truecase import truecase_corpuses\n",
        "from src.preprocessing.preprocess import construct_model_data\n",
        "from src.preprocessing.corpus_utils import read_corpus, get_references\n",
        "from src.import_configs import import_configs\n",
        "from src.train import train, load_checkpoint\n",
        "from src.predict import predict\n",
        "from src.evaluate import evaluate\n",
        "from src.model_utils import load_pretrained\n",
        "from src.translate import translate\n",
        "import stanza\n",
        "from subword_nmt.apply_bpe import BPE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJNv2c22F3U-"
      },
      "source": [
        "# step 1 - tokenize corpuses, and tag with morphological data.\n",
        "#apply_stanza_processors(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", path=corpus_path)\n",
        "apply_stanza_processors(\"dev.en\", path=corpus_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFNueYblGugP"
      },
      "source": [
        "# step 2 - true-case corpuses using linguistic heuristics that leverage morphological\n",
        "# data produced by morphological data tagger.\n",
        "#truecase_corpuses(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", corpus_path=corpus_path)\n",
        "truecase_corpuses(\"dev.en\", corpus_path=corpus_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPdPJEq1dVkg"
      },
      "source": [
        "# import vocab, training, and model hyperparameter settings from configuration files.\n",
        "hyperparams = import_configs(config_path=config_path)\n",
        "print(hyperparams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZH_5k34It3l"
      },
      "source": [
        "# step 3 - segment words of corpuses into subwords (skip this cell if using a word-level vocabulary).\n",
        "num_merge_ops = hyperparams[\"num_merge_ops\"]\n",
        "vocab_threshold = hyperparams[\"vocab_threshold\"]\n",
        "truecased_path = corpus_path + 'truecased/'\n",
        "segmented_path = corpus_path + 'subword_segmented/'\n",
        "\n",
        "!bash ./src/preprocessing/subword_joint.sh $num_merge_ops $vocab_threshold \"$truecased_path\" \"$segmented_path\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44V9gcJFIxIo"
      },
      "source": [
        "# step 4 - build intelligently batched sets of tensors that can be directly passed to model.\n",
        "#construct_model_data(\"train.de\", \"train.en\", \"dev.de\", hyperparams=hyperparams, corpus_path=corpus_path+'subword_segmented_more_val/', reference_path=corpus_path, checkpoint_path=checkpoint_path)\n",
        "construct_model_data(\"train.de\", \"train.en\", \"dev.de\", \"dev.en\", hyperparams=hyperparams, corpus_path=corpus_path+'subword_segmented/', reference_path=corpus_path, checkpoint_path=checkpoint_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJy_MNEYI8rA"
      },
      "source": [
        "# step 5 - instantiate and train model.\n",
        "#model, loss = train(checkpoint_path=checkpoint_path)\n",
        "model, loss = train(checkpoint_path=checkpoint_path, threshold=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drtmLg6bxkRk"
      },
      "source": [
        "# prepare pretrained model for end-to-end inference.\n",
        "stanza.download(lang='de', processors='tokenize,mwt,pos')\n",
        "stanza_de_processor = stanza.Pipeline(lang='de', processors='tokenize,mwt,pos', tokenize_no_ssplit=True, tokenize_batch_size=64, mwt_batch_size=200, pos_batch_size=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEGpCZHaSp9g"
      },
      "source": [
        "# step 6 - evaluate test set predictions.\n",
        "#translator, model_data = load_pretrained(checkpoint_path)\n",
        "#translator, model_data = load_pretrained(checkpoint_path, name=\"best_bleu_model\")\n",
        "translator, model_data = load_pretrained(checkpoint_path, name=\"best_loss_model\")\n",
        "\n",
        "src_word_to_idx = model_data[\"src_word_to_idx\"]\n",
        "idx_to_trg_word = model_data[\"idx_to_trg_word\"]\n",
        "\n",
        "# optional: first observe beam search predictions of best model on dev set:\n",
        "# (during training, used greedy search).\n",
        "# bleu should improve by ~1.5\n",
        "dev_batches = model_data[\"dev_batches\"]\n",
        "dev_references = model_data[\"references\"]\n",
        "\n",
        "#translator.decoder.set_inference_alg(\"beam_search\", 0.0)\n",
        "\n",
        "dev_translations, preds_time, post_time = predict(translator, dev_batches, idx_to_trg_word, checkpoint_path)\n",
        "dev_bleu = evaluate(dev_translations, dev_references)\n",
        "print(round(dev_bleu, 2))\n",
        "print(preds_time)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# evaluate model on test set.\n",
        "#bpe = BPE(open(corpus_path + 'subword_segmented_more_val/bpe_codes', 'r'), vocab=set(src_word_to_idx))\n",
        "bpe = BPE(open(corpus_path + 'subword_segmented/bpe_codes', 'r'), vocab=set(src_word_to_idx))\n",
        "\n",
        "#test_path = path + 'data/iwslt/en-de/'\n",
        "test_path = corpus_path\n",
        "test_set = read_corpus('test.de', path=test_path)\n",
        "test_references = get_references(path=test_path, dev=False)\n",
        "translations = translate(test_set, stanza_de_processor, translator, src_word_to_idx, idx_to_trg_word, bpe)\n",
        "test_bleu = evaluate(translations, test_references)\n",
        "print(round(test_bleu, 2))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4knYQ1Mx71G"
      },
      "source": [
        "for beta in [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]:\n",
        "  translator.decoder.set_inference_alg(\"beam_search\", beta)\n",
        "  translations = translate(test_set, stanza_de_processor, translator, src_word_to_idx, idx_to_trg_word, bpe)\n",
        "  bleu = evaluate(translations, test_references)\n",
        "  print(f\"beta: {beta}, bleu: {bleu}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEAsgWD9drH5"
      },
      "source": [
        "# b) run unit tests to show correctness of model implementations\n",
        "# can run each separately, or discover and run all at once (see below)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuHPjynIdsRy"
      },
      "source": [
        "# allow ~5 min to run all model variant tests, each of which trains for 100 epochs.\n",
        "!python -m pytest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyxUz19gdwnk"
      },
      "source": [
        "!python -m pytest unittests/test_batches.py # ensure intelligent batching procedure is correct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYlVCoG8d2QB"
      },
      "source": [
        "!python -m pytest -s -v unittests/test_model.py::test_default_word_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I5U-E2Pd3w5"
      },
      "source": [
        "!python -m pytest -s -v unittests/test_model.py::test_default_subword_model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}